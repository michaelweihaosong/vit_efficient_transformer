# MNIST

# ViT non performer 
python vit_performer.py \
--kernel non_performer \
> ./logs/MNIST/MNIST_vit_non_performer.log

# ViT performer relu
python vit_performer.py \
--kernel relu \
--nb_features 0 \
> ./logs/MNIST/MNIST_vit_performer_relu_nb_features_0_feature_redraw_interval_None.log

# ViT performer quad
python vit_performer.py \
--kernel quad \
--nb_features 0 \
> ./logs/MNIST/MNIST_vit_performer_quad_nb_features_0_feature_redraw_interval_None.log

# ViT performer x^4
python vit_performer.py \
--kernel x^4 \
--nb_features 0 \
> ./logs/MNIST/MNIST_vit_performer_x^4_nb_features_0_feature_redraw_interval_None.log




# ablation

# ViT performer softmax with redraw

python vit_performer.py \
--kernel softmax \
--nb_features 512 \
--redraw 1000 \
> ./logs/MNIST/MNIST_vit_performer_softmax_nb_features_512_feature_redraw_interval_1000.log

python vit_performer.py \
--kernel softmax \
--nb_features 64 \
--redraw 1000 \
> ./logs/MNIST/MNIST_vit_performer_softmax_nb_features_64_feature_redraw_interval_1000.log



# ViT performer softmax no redraw

python vit_performer.py \
--kernel softmax \
--nb_features 512 \
> ./logs/MNIST/MNIST_vit_performer_softmax_nb_features_512_feature_redraw_interval_None.log

python vit_performer.py \
--kernel softmax \
--nb_features 64 \
> ./logs/MNIST/MNIST_vit_performer_softmax_nb_features_64_feature_redraw_interval_None.log





# Caltech256
reformer n_bucket_size 4 n_hashes 1
reformer n_bucket_size 8 n_hashes 1
performer relu feature 64 redraw 1000
performer softmax feature 64 redraw 1000
nystromformer 16 2

# Caltech101
reformer n_bucket_size 4 n_hashes 1
reformer n_bucket_size 8 n_hashes 1
reformer n_bucket_size 16 n_hashes 1
performer relu feature 64 redraw 1000
performer softmax feature 64 redraw 1000
nystromformer 16 2


























# Caltech256 patch_size 8

# ViT transformer
python vit_efficient_transformer.py \
--model non_performer \
--dataset Caltech256 \
--batch-size 32 \
--test-batch-size 256 \
> ./logs/Caltech256/Caltech256_vit_transformer.log

# ViT performer relu
python vit_efficient_transformer.py \
--model relu \
--nb_features 0 \
--dataset Caltech256 \
--batch-size 32 \
--test-batch-size 256 \
> ./logs/Caltech256/Caltech256_vit_performer_relu_nb_features_0_feature_redraw_interval_None.log

# ViT linformer
python vit_efficient_transformer.py \
--model linformer \
--dataset Caltech256 \
--batch-size 32 \
--test-batch-size 256 \
--k_linformer 64 \
--one_kv_head \
> ./logs/Caltech256/Caltech256_vit_linformer_k_64_one_kv_head_True.log

python vit_efficient_transformer.py \
--model linformer \
--dataset Caltech256 \
--batch-size 32 \
--test-batch-size 256 \
--k_linformer 64 \
> ./logs/Caltech256/Caltech256_vit_linformer_k_64_one_kv_head_False.log

# ViT reformer
python vit_efficient_transformer.py \
--model reformer \
--dataset Caltech256 \
--batch-size 32 \
--n_bucket_size 32 \
--n_hashes 1 \
--test-batch-size 256 \
> ./logs/Caltech256/Caltech256_vit_reformer_n_bucket_size_32_n_hashes_1.log


# ViT longformer
python vit_efficient_transformer.py \
--model longformer \
--dataset Caltech256 \
--batch-size 32 \
--attention_window 128 \
--test-batch-size 256 \
> ./logs/Caltech256/Caltech256_vit_longformer_attention_window_128.log

# ViT performer quad
python vit_efficient_transformer.py \
--kernel quad \
--nb_features 0 \
--dataset Caltech256 \
--batch-size 32 \
--test-batch-size 256 \
> ./logs/Caltech256/Caltech256_vit_performer_quad_nb_features_0_feature_redraw_interval_None.log

# ViT performer x^4
python vit_efficient_transformer.py \
--kernel x^4 \
--nb_features 0 \
--dataset Caltech256 \
--batch-size 32 \
--test-batch-size 256 \
> ./logs/Caltech256/Caltech256_vit_performer_x^4_nb_features_0_feature_redraw_interval_None.log




# ablation

# ViT performer softmax with redraw

python vit_performer.py \
--kernel softmax \
--nb_features 384 \
--redraw 1000 \
--dataset Caltech256 \
--batch-size 32 \
--test-batch-size 256 \
> ./logs/Caltech256/Caltech256_vit_performer_softmax_nb_features_384_feature_redraw_interval_1000.log

python vit_performer.py \
--kernel softmax \
--nb_features 64 \
--redraw 1000 \
--dataset Caltech256 \
--batch-size 32 \
--test-batch-size 256 \
> ./logs/Caltech256/Caltech256_vit_performer_softmax_nb_features_64_feature_redraw_interval_1000.log



# ViT performer softmax no redraw

python vit_performer.py \
--kernel softmax \
--nb_features 384 \
--dataset Caltech256 \
--batch-size 32 \
--test-batch-size 256 \
> ./logs/Caltech256/Caltech256_vit_performer_softmax_nb_features_384_feature_redraw_interval_None.log

python vit_performer.py \
--kernel softmax \
--nb_features 64 \
--dataset Caltech256 \
--batch-size 32 \
--test-batch-size 256 \
> ./logs/Caltech256/Caltech256_vit_performer_softmax_nb_features_64_feature_redraw_interval_None.log
























# Caltech101 patch_size 8

# ViT transformer
python vit_efficient_transformer.py \
--model transformer \
--dataset Caltech101 \
--batch-size 32 \
--test-batch-size 256 \
> ./logs/Caltech101/vit_transformer_patch_size_8.log

# ViT performer relu
python vit_efficient_transformer.py \
--model relu \
--nb_features 0 \
--dataset Caltech101 \
--batch-size 32 \
--test-batch-size 256 \
> ./logs/Caltech101/vit_performer_patch_size_8_relu_nb_features_0_feature_redraw_interval_None.log

# ViT linformer
python vit_efficient_transformer.py \
--model linformer \
--dataset Caltech101 \
--batch-size 32 \
--test-batch-size 256 \
--k_linformer 64 \
--one_kv_head \
> ./logs/Caltech101/vit_linformer_patch_size_8_k_64_one_kv_head_True.log

python vit_efficient_transformer.py \
--model linformer \
--dataset Caltech101 \
--batch-size 32 \
--test-batch-size 256 \
--k_linformer 64 \
> ./logs/Caltech101/vit_linformer_patch_size_8_k_64_one_kv_head_False.log

# ViT reformer
python vit_efficient_transformer.py \
--model reformer \
--dataset Caltech101 \
--batch-size 32 \
--n_bucket_size 32 \
--n_hashes 1 \
--test-batch-size 256 \
> ./logs/Caltech101/vit_reformer_patch_size_8_n_bucket_size_32_n_hashes_1.log

# ViT longformer
python vit_efficient_transformer.py \
--model longformer \
--dataset Caltech101 \
--batch-size 32 \
--attention_window 128 \
--test-batch-size 256 \
> ./logs/Caltech101/vit_longformer_patch_size_8_attention_window_128.log








